\section{Introduction}
\label{sec:introduction}

\subsection{Research Questions}

\subsection{Approach}

\subsection{Related Work}
\label{sec:related-work}

Early work on transfer learning for reinforcement learning mostly relied on human intervention to create a mapping between source and target tasks \citep[e.g.][]{taylor2007cross}. \citet{taylor2007cross}, for example, developed a method called \textit{Rule Transfer}. Their algorithm learns a policy in the source task that gets transformed into rules, serving as advise to the agent when training in the new environment. To use these rules in the target task, hand-coded translation functions were applied. In contrast, \citet{taylor2008autonomous} published the first system that automatically mapped source and target task. They use little data from a short exploration period in the target task to approximate a one-to-many mapping between the state and action space. This is achieved by comparing all possible state-state and action-action pairs and choosing the ones with the smallest MSE when predicting the next action using neural networks trained on the target task observations. While their method effectively facilitated learning in the target task, it needs to be noted that transfer was performed on modifications of the same task. Hence, they were fairly similar and there was no attempt to tackle cross-domain transfer. 

In the following sections ...
