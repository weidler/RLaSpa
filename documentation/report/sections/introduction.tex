\section{Introduction}
\label{sec:introduction}

% topic introduction & motivation
Reinforcement learning (RL) addresses machine learning problems where complex tasks with minimal feedback \citep{taylor2007cross} need to be solved by agents in a sequence of actions as reactions to a changing environment. RL methods were successfully applied in different areas such as game playing \citep{silver2016mastering}, robotics \citep{levine2016end} and resources management \citep{mao2016resource}. 
Common approaches to RL create task-specific solutions. This means that when facing a new environment, the agent needs to restart learning. This limitation calls for transfer learning, which aims at transferring previously gained knowledge across different tasks. Besides reducing time-consuming re-training of similar tasks, transfer learning can potentially enable RL agents to operate in environments where learning is otherwise too challenging \citep{barreto2018transfer} or expensive. In RL, transfer faces two key challenges. Firstly, even if tasks are similar, state and action spaces may be of \textit{different dimensions} between them. Secondly, the agent's policy needs to be not \textit{overfitted} to the source task, in order to make the underlying knowledge reusable. % example pathing?
The first issue can be solved by only taking information into account that is available in all tasks, such as pixels. In fact, this is a rather obvious requirement: The agent can only learn transferable knowledge, if it is presented with information that allows it to find common patterns. The second problem, learning generalizable policies, is more challenging and requires models that guide the agent towards a transferable policy. 

% approach
In this work, we explore the possibility of using a latent representation created by a deep neural network to achieve transfer learning in RL. Deep learning methods can discover representations that are abstract and domain-invariant \citep{bengio2012deep, ganin2014unsupervised}. This characteristic can be utilized for transfer learning in RL. Instead of presenting the agent with the raw state of an environment, a representation module first encodes the pixels into a latent space using a stack of convolutional layers. In contrast to previous research \citep[see e.g.][]{DQN, DuelingDQN}, we not only train this representation on the given task but also calculate the loss using an autoencoder architecture. This guides the encoder into creating representations that can be used to reconstruct the given image. We hypothesize such representations to encode important features in the resulting latent space and therefore make it easier to generalize to new tasks.

% research questions & contributions
More specially, we seek to answer the following research questions:
\begin{itemize}
	\item Can the proposed autoencoder architectures capture commonalities of several similar yet different tasks, such that an RL agent can use the encoded representation to learn to perform all these tasks?
	\item After learning a task based on the encoded representation, can the agent learn to perform a previously unseen but similar task more easily (e.g. taking less time to train)? Or can the agent do so without learning?
\end{itemize}

% outlook
In the following sections, we first review selected previous works relevant to our approach. We then introduce our own model and describe the different techniques used in the final system in detail (Section \ref{sec:approach}). In Section \ref{sec:experiments} the experimental setup is described, followed by the results in Section \ref{sec:results}. We discuss these results in Section \ref{sec:discussion} and finally draw a conclusion.