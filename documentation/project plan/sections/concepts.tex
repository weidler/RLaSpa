\section{Concepts and approach}

The development of the project will require the use of different deep learning algorithms in order to tackle the final goal. Tasks like the feature extraction from the Atari frames, the generation of the latent space and the finding and training of the agent policy will be handled by deep learning techniques.

When dealing with Atari games, the problem of constructing features-vectors from raw pixels can be addressed using \textbf{Convolutional neural networks (CNNs)}.
CNNs, are a class of neural networks designed to process ``grid-like topology data'' (\cite{goodfellow2016deep}) like images or sounds signals. CNNs allow to ``abstract'' features from the raw pixels matrix using an operation called \textit{convolution}, a special kind of linear operation. Each feature will be represented by a \textit{filter}: a vector of weights and the respective bias that are incrementally adjusted to ``learn'' how to correctly extract that specific feature (e.g. a square shape). 
Furthermore, by using gameplay frames as input for the CNN, we assure that each task to later embed in the latent space has the same data structure for the state space.  

In order to generate the latent space, \textbf{autoencoders} will be used. An autoencoder is a neural network formed by two parts: the encoder, that converts the input to a dense and smaller representation and the decoder, that rebuild the input from the representation. The main application for autoencoders is the dimensionality reduction (\cite{Hinton504}), where the encoder learns to preserve the meaningful attributes of the input and generates a lower dimensional representation that is saved in the \textbf{latent space}. 

As we work with different tasks, the latent space will be the place where we aim to save a generalized representation of the state spaces. To create a continuous latent space, that could allow transfer among task, a \textbf{variational autoencoder (VAE)} will be used. This kind of neural network use probability distributions to describe each feature, allowing interpolation and random sampling in the data. Moreover, VAEs are trained with \textbf{gradient based methods} which gives a better control over the latent space representation (\cite{goodfellow2016deep}) and can be used to generate representations with disentangled factors (\cite{2016arXiv160605579H}).

To achieve transfer learning, the agent will learn from the generalisation of the states stored in the latent space using deep reinforcement learning techniques. This algorithms will be used with the objective of creating an agent that will be capable of detecting similar states and situations that occurs on tasks and act in a similar way in each of them. Two approaches will be taken in consideration.

On the one hand, \textbf{deep Q-learning (DQN)} makes use of a special type of neural network called deep Q-network which is used to approximate the reward based on the state, the Q value. The objective of this method is to find a function that maximize that value, which is the expected reward after taking a specific action in a certain  state.  


On the other hand, \textbf{policy gradients} follow a simpler approach and try to learn the policy function that maps the state to the action directly. This method learns for past trajectories, increasing the probability of that actions which in the past returned good results for the agent.




% Concepts:
% \begin{itemize}
% \item Existing Technologies
% \item Methods
% \item Terminology
% \end{itemize}

% Approach:
% \begin{itemize}
% \item What will you be doing with these technologies?
% \item How will you apply them?
% \end{itemize}