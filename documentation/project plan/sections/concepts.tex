\section{Concepts and approach}

\textbf{Deep learning (DL)} is a specific subfield of machine learning that aim to learn representations of the world from sets of high level features built by the composition of lower level ones. The main aspect of deep learning is that these set of features, called layers, are learned from raw data through general-purpose procedures and without the intervention of humans. 

The algorithms that allow this kind of learning are the \textbf{artificial neural networks} which are a mathematical function that maps a set of inputs values to outputs values (\cite{goodfellow2016deep}). Such function is a composition of many simpler functions that provides several levels of abstractions. 

\textbf{Convolutional neural networks (CNNs)} are a category of artificial neural networks designed to process ``grid-like topology data'' (\cite{goodfellow2016deep}) like images or sounds signals. Its architecture, that take advantage of the topology of the input, arrange the neurons in three dimensions: width, height and depth. 

The name of this kind of neural networks comes from the operation in which they are based, the convolution, which is a special kind of linear operation. CNNs architecture is formed by four main layers:

\begin{itemize}
    \item Convolutional layer.
    \item ReLu layer.
    \item Pooling layer.
    \item Classification layer.
\end{itemize}

The convolution layer is the main part of the CNNs, being in charge of detecting the features in the data by the use of filters. To improve the speed of the training, convolution layers are followed by the rectified linear units (ReLu) layer, that acts as a rectifier function.

The functionality of the pooling layer is reduce the spatial size of the representation with the objective of decrease the parameters in the network and the computation required. Finally, the last part of the network is the fully connected layer, which performs the classification task.

An \textbf{autoencoder} is neural networks trained to copy its input to the output and which is formed by two parts: the encoder, that converts the input to a dense and smaller representation and the decoder, that rebuild the input from the representation. The main application for autoencoders is the dimensionality reduction (\cite{Hinton504}), where the encoder learns to preserve the meaningful attributes of the input and generates a lower dimensional representation that is saved in the \textbf{latent space}.

One of the limitations of generic autoencoders is that the generated latent space is based in features of the data and may not be continuous. This lack of continuity works well for the replication of the input but produce unrealistic outputs in generative tasks or with inputs that has not been previously observed.

In order to create a continuous latent space, \textbf{Variational autoencoders (VAEs)} use probability distributions to describe each attribute, allowing interpolation and random sampling in the data. Moreover, VAEs are trained with \textbf{gradient based methods} which gives a better control over the latent space representation (\cite{goodfellow2016deep}) and generates representations with disentangled factors (\cite{2016arXiv160605579H}).

\textbf{Latent Space (LS)} is the space in which information generated by the encoder lies. It contains the compressed representation of the input that can be used by the decoder to regenerate the data. 

\hrulefill

How will you be tackling/solving this?



Concepts:
\begin{itemize}
\item Existing Technologies
\item Methods
\item Terminology
\end{itemize}

Approach:
\begin{itemize}
\item What will you be doing with these technologies?
\item How will you apply them?
\end{itemize}