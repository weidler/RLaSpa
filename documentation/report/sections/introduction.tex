\section{Introduction}
\label{sec:introduction}

% topic introduction & motivation
Reinforcement learning (RL) addresses machine learning problems where complex tasks with minimal feedback \citep{taylor2007cross} need to be solved by agents in a sequence of actions as reactions to a changing environment. RL methods were successfully applied in different areas such as game playing \citep{silver2016mastering}, robotics \citep{levine2016end} and resource management \citep{mao2016resource}. 
Most common approaches to RL is to create task-specific solutions. This means that when facing a new environment, the agent needs to restart learning. This limitation calls for transfer learning, which aims at transferring previously gained knowledge across different tasks. Besides reducing time-consuming re-training of similar tasks, transfer learning can potentially enable RL agents to operate in environments where learning is otherwise too challenging \citep{barreto2018transfer} or expensive. In RL, transfer faces two key challenges. Firstly, albeit the tasks are similar, state and action spaces may be of \textit{different dimensions} between them. Secondly, the agent's policy need to be not \textit{overfitted} to the source task, so to make the underlying knowledge reusable. % example pathing?
The first issue can be solved by only taking information into account that is available in all tasks, such as pixels\footnote{Obviously, this requires the task to be visualizable.}. In fact, this is a rather obvious requirement: The agent can only learn transferable knowledge, if it is presented with information that allows it to find common patterns. Learning generalizable policies is more challenging and requires models that guide the agent towards a transferable policy. 



% approach
In this work, we explore the possibility of using a latent representation created by a deep neural network to achieve transfer learning in RL. Deep learning methods can discover representations that are abstract and domain-invariant \citep{bengio2012deep, ganin2014unsupervised}. This characteristic can be utilized for transfer learning in RL: Instead of presenting the agent with the raw state of an environment, a representation module first encodes the pixels into a latent space using a stack of convolutional layers. In contrast to previous research \citep[see e.g.][]{DQN, DuelingDQN}, we not only train this representation on the given task but also calculate the loss using an autoencoder architecture. This guides the encoder into creating representations that can be used to reconstruct the given image. We hypothesize such representations to make the resulting latent space to encode important features and therefore easier to generalize to new tasks.

% research questions & contributions
More specially, we seek to answer the following research questions:
\begin{itemize}
	\item How to create a latent space that captures commonalities of several similar yet different tasks, such that an RL agent can learn to perform all these tasks only based on the latent representation?
	\item After learning a task based on the latent representation, can the agent learn to perform a previously unseen but similar task more easily (e.g. taking less time to train)? Or can the agent do so without learning?
\end{itemize}

% outlook
In the following we first review previous research on reinforcement learning relevant to our approach. We then introduce our approach and describe the different techniques used in the final system in detail. In Section \ref{sec:experiments} the experimental setup is described, followed by the results in section \ref{sec:results}. We discuss these results in Section \ref{sec:discussion} and draw a conclusion in Section \ref{sec:conclusion}.