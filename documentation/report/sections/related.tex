\section{Related Work}
\label{sec:related-work}
We begin by summarizing recent work on RL with a focus on those publications that tackle tasks with visual input. Subsequently, an overview on transfer learning in the field of RL is given. 

\subsection{Reinforcement Learning for Visual Input}
Advances in Deep Learning (DL) had a great impact on the different areas of machine learning \citep{deep_learning_development}, including reinforcement learning. The main property of DL is the possibility of extracting low-dimensional representations, often called latent features, from raw or high-dimensional data like sound, images or text. This characteristic has allowed the use of RL algorithms in environments that were previously intractable due to high-dimensional or continuous states and actions.

The integration of DL methods into RL has defined the field \textit{Deep Reinforcement Learning} (DRL) whose first breakthrough came with the development of the Deep Q-Network (DQN) \citep{DQN}, an algorithm capable of playing Atari 2600 video games given visual input. This RL agent used a neural network to extract the information from the raw pixels of the screen and the game reward to learn a policy that ultimately outperformed humans in different games. The DQN approach also uses the Experience Replay \citep{replay_memory_oc} to improve the learning process. This mechanism stores the situations that the agent previously encountered as memories and randomly samples them to train the network. The sampling of memories is meant to remove correlation between training instances and to avoid forgetting previous experiences, which should facilitate learning.

Since the publication of the original DQN algorithm, many improvements have been proposed. The prioritized experience replay \citep{prioritized_memory} was introduced to consider the difference between the network prediction and the reality of the environment after the agent interaction when sampling from the memory. With the idea that some experiences are more useful than others to learn, the prioritized rexperience replay weights them with respect to this loss, prioritizing the ones that have a bigger loss.

To improve the learning process in DQNs, which may overestimate the value of the actions, the use of Double Deep Q-Networks (DDQN) \citep{DDQN} was proposed. DDQNs make use of a second network that is updated after fixed time intervals with a copy of the original network. The overestimation bias is mitigated by the introduction of the second DQN. One network functions as the actor and the other predicts the state-action pairs. Moreover, by fixing the estimations for longer periods during the training, the learning process is stabilized since the target values are not changed immediately when updating the current model.

Additionally, different architectures have been proposed to adapt to specific situations: to improve exploration \citep{noisy_dqn, hierarchical_dqn}, to benefit from different estimations \citep{DuelingDQN, distributional_dqn} and to join different improvements in one algorithm \citep{rainbow}. 

In this project we use the Double Deep Q-Network algorithm \citep{DDQN} that we adapt in order to work with an additional representation module. 

\subsection{Transfer Learning for Reinforcement Learning}
% Transfer Learning for RL
Early work on transfer learning for reinforcement learning mostly relied on human intervention to create a mapping between source and target tasks. \citet{taylor2007cross} developed a method that learns a policy on a source task that gets transformed into rules, serving as advise to the agent when training on the new environment. Hand-coded translation functions are required to use these rules in the target task.
Later \citet{taylor2008autonomous} developed the first system that automatically maps source and target task. 
They use little data from a short exploration period in the target task to approximate a one-to-many mapping between the state and action spaces.
%This is achieved by comparing all possible state-state and action-action pairs and choosing the ones with the smallest MSE when predicting the next action using neural networks trained on the target task observations. 
%While their method effectively facilitated learning in the target task, it needs to be noted that transfer was performed on modifications of the same task. 

More recent works deal with less similar source and target tasks, but still require some form of external alignment.
For example, \citet{parisotto2015actor} used an actor-mimic approach that demonstrated positive results in generalization from different Atari games.
\citet{gupta2017learning} created a latent representation for source and target tasks based on pairs of corresponding states found via time-based alignment or dynamic time warping.
%\citet{parisotto2015actor} trained an agent to learn multiple related games of the Atari Learning Environment simultaneously to later generalize from the learned experiences. The training was done by teaching the agent to mimic an expert and then doing a feature regression of the learned mimicking. 
%This can be seen as telling the agent what to do and later telling him why he should do it this way. They proposed to use this actor-mimic approach as a pre-training to increase learning speed on a set of tasks.
%\citet{gupta2017learning} used a proxy task learned in both the source and target domains, and a test task where transfer should occur. Firstly, with the proxy task, pairs of corresponding states are found using time-based alignment or dynamic time warping. Based on these state pairs, a common latent state space is learned by minimizing reconstruction errors and pairwise distances. In the test task, to incentivize policy transfer from source to task, the distance to source optimal policy in the common space is incorporated in the reward function.

More recently, model-agnostic approaches have been developed as well.
\citet{MAML} proposed a meta-learning model that is able to perform few-shot learning.
%capable of adapting to and performing in different tasks that are independent from each other.
%Using gradient descent, the meta-learner is trained over a wide range of them with the objective of constructing a general latent representation that allows the agent to behave correctly in the known activities and, also, that can be quickly adapted for new unseen tasks, using few examples.
For tasks where 
%the reward cannot be found in a few steps and exploration is needed, 
few-shot learning is challenging, this approach was extended with stochastic exploration in the later work by \citet{MAESN}.
%This algorithm uses the policy and the latent space learned in previous tasks in conjunction with noise to generate better informed exploration strategies that accelerates the learning in unseen environments.


% \citet{mnih2016asynchronous} used asynchronous gradient descent to train deep neural networks. This framework is lightweight so that it can run on a CPU instead of a GPU. They "execute multiple agents in parallel on multiple instances of the environment" \citet{mnih2016asynchronous}. This stabilized learning and reduced the training time. The reduction in training time was roughly linear to the number of processes. Asynchronous advantage actor-critic (A3C) achieved new state-of-the-art performances in 57 Atari games.

% \citet{andrychowicz2017hindsight} say that one of the biggest challenges in RL are sparse rewards. They constructed an algorithm that learns from undesired results as well as from desired results. This way the agent can learn from more experiences and thus constructing a reward function is not necessary. Constructing a good reward function is challenging (\citet{ng1999policy}) and can be complicated (\citet{popov2017data}). They showed that with their approach tasks were able to be learned that previously were not possible. Furthermore, they proposed to train an agent "on multiple goals even if we care only about one of them." \citet{andrychowicz2017hindsight}.

% RL for visual tasks/DL for RL
% PUT HERE DQN/DDQN/..., that is, general work on visual RL

% \subsubsection{Deep Q-Network}
% One of the drawbacks of table-based Q-learning occurs in environments with large state spaces.
% Maintaining and updating the values of all possible states is memory intensive and requires a great amount of training data.
% An alternative that avoids this problem is function approximation. 
% We use a Deep Q-Network (DQN) \citep{DQN}, which is a Q-learning algorithm that uses deep neural networks to approximate the state Q-values of each action.

% Experience replay is used to improve the learning process in DQN. 
% This mechanism stores the situations that the agent previously encountered as ``memories'' and randomly samples them to train the network. 
% The sampling of memories is meant to remove correlation between training instances and avoid forgetting previous experiences, facilitating the learning.



% \subsubsection{Double Deep Q-Network}
% Double Deep Q-Network (DDQN) \citep{DDQN} is an improvement to DQN that stabilizes the target Q-values to be predicted. In DQN, the maximization step taken to calculate the next Q-value (Equation \ref{eq:dqn_td}) can lead to inaccurate predictions that generates overestimation bias, affecting the learning process.

% \begin{equation}\label{eq:dqn_td}
%      Q(s,a) = r(s,a) + \gamma max_{a}Q(s',a)
% \end{equation}

% By decoupling the selection of the action from the value evaluation, as in equation \ref{eq:ddqn_td}, DDQN addresses this overestimation problem. Two networks are used in this process: the original DQN, used to choose the action that maximizes the Q-value and a target network, $Q_{t}$, that calculates the estimation with the given action.

% % This is done by the use of a target network, which will estimate the next Q-value after choosing and action with the DQN network. is updated with the trained network every certain number of steps.

% \begin{equation}\label{eq:ddqn_td}
%      Q(s,a) = r(s,a) + \gamma Q_{t}(s,argmax_{a}Q(s',a))
% \end{equation}

% More specifically, the DQN network is used and updated during the training meanwhile the target network is a snapshot of the first, that is made every certain number of steps. This procedure makes the target function fixed between updates, allowing a more stable training.


% % More specifically, the target network is kept fixed for a certain number of steps after which is updated with the DQN weights, that keeps changing every step.

% %DQN state, contrary to in DQN where it changes every step.

% \iffalse
% \begin{itemize}
% 	\item DDQN  also has prioritized memory 
% 	%Dueling DQN \citep{DuelingDQN}: state value, mix between q=learning and state action.
% 	\item DQN update policy every time. 
% 	\item continuously changing policy, estimate q-value changes every time.
% 	target network updated every 100 steps. prediction of q-values is fixed. otherwise there is ``more bias''?
% \end{itemize}
% \fi
