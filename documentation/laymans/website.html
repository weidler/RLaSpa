<!DOCTYPE html>
<html>

<head>
    <title>Reinforcement Learning in Latent Space</title>
</head>

<body>
    <h2>Reinforcement Learning in Latent Space</h2>

    Student(s): Adrian Rodriguez Grillo, Danni Liu, Alessandro Scoppio, Kevin Trebing, Tonio Weidler;
    Supervisor(s): Kurt Driessens;
    Semester: 2017-2018;

    <figure><img style="float: left; padding-right: 20px;" src="http://<url to image>" alt="" width="458" height="300" />
        <figcaption>Fig 1. -  An agent learning a path from its current location to a goal on the left map.
            When applying the learned knowledge to the map on the right, the path is insufficient.
            Instead, the agent should learn an abstract concept of avoiding obstacles and reaching a goal.
        </figcaption>
    </figure>

    <h4>Problem statement and motivation:</h4>
    <p align=justify>
        <strong>Reinforcement learning</strong> is a subfield in artificial intelligence that shares many commonalities with human learning.
        In reinforcement learning, a virtual agent is set free in an unknown environment to solve a task,
        for example walking towards a destination. Whilst exploring, it will receive rewards or punishments depending on its behaviour.
        These are used to reinforce desired or suppress undesired behaviour. In the example of reaching a destination, 
        this could be whether it moves towards or away from it. The agent will act via trial-and-error, 
        attempting to collect the maximum possible reward. By repeatedly doing different kinds of actions in the environment, 
        the agent explores and thereby accumulates knowledge about what to do given a particular situation. 
        The agent measures the quality of an action based on its expectations about the rewards it would lead to in the future.
        The behavior that the agent learns is called a policy, which describes at every step what he should do. After its learning phase, 
        the agent should have inferred the best behaviour from the rewards and punishments it received. 
        Unlike humans and animals, the virtual agent can explore the unknown environment very quickly, thanks to fast computer simulations. 
        However, when facing a new task, it cannot take advantage of previously learned knowledge and has to explore and learn again from scratch. 
        In this project, the goal is to enable the agent to use previously learned knowledge on a new task, or transfer learning. 
        Transferring knowledge is what allows humans and animals to solve problems they never faced before and quickly adapt to them. 
        Imagine a human that learns how to build a chair from scratch. If he masters this skill, we can expect him to quickly find out how to build a table,
        applying the knowledge he gained from building chairs. A virtual agent can not do this without expensive and task-specific help from its creator. 
        In fact, an essential part of the learning capabilities of humans is being able to construct abstractions, or knowledge representations.
        Neural networks are mathematical models designed to construct such abstractions. The abstraction is often referred to as latent space, 
        because it is supposed to reveal characteristics of the input that are not directly visible. 
        This latent representation would eventually embed general knowledge about the environment. When the agent learns how to behave in an environment based on abstracted information, we hope that it will be able to utilize the gained knowledge in new tasks by applying the same abstraction technique. 
        Imagine a virtual agent that needs to find a way from its current location to a destination, but this time, there are obstacles blocking the direct path. The agent can easily solve this problem by learning for each position on the map what action brings it closest to the target. However, only remembering the best path will not help it, when we change the obstacles on the map. If a new obstacle blocks the original solution, the agent will never reach its goal. See figure 1 for an illustrative description of this situation. What we want to achieve, is that the agent learns to abstract its visual input. Instead of remembering one path, it should learn the concept of tracing around objects and moving in the direction of the goal. Optimally, this would make the arrangement of obstacles irrelevant and the agent becomes capable of finding its way - even on unknown maps.
        We aim at finding a way of guiding the abstraction process such that it both contains information specific enough to solve learned tasks but also general enough to transfer the knowledge to new tasks. We will investigate and compare multiple approaches to this problem. To evaluate them, we collect sets of tasks that are similar enough to allow knowledge transfer. We conduct first experiments on very easy problems, but extend our scope to more difficult and potentially diverse tasks as soon as we achieve good results on the former.
        Being able to reuse previously learned knowledge would help to advance the field of reinforcement learning by potentially making training an agent more efficient, due to its capability of leveraging knowledge from previous tasks. This could make it possible to solve tasks that are for now very ineffective to solve.

    </p>
    <p align=justify>

    </p>

    <figure>
        <img style="text-align: center;" src="http://<url to image>" alt="" />
        <figcaption>Fig 2. - ???.</figcaption>
    </figure>

    <figure>
        <img style="text-align: center;" src="http://<url to image>" alt="" />
        <figcaption>Fig 3. - ???.</figcaption>
    </figure>

    <h4>Research questions/hypotheses:</h4>
    <ul>
        <li>Could the states of different tasks be jointly represented in the same latent space?</li>
        <li>?</li>
        <li>??? ?</li>
    </ul>

    <h4>Main outcomes:</h4>
    <ul>
        <li>???.</li>
        <li>???.</li>
    </ul>

    <h4>References:</h4>
    Kitano, H., Asada, M., Kuniyoshi, Y., Noda, I., &amp; Osawa, E. (1997). Robocup: the robot world cup initiative.
    Proceedings of the first international conference on Autonomous agents, 340-347.

    <h4>Downloads:</h4>
    <a href=â€???â€ target=â€_blankâ€>Final report</a>
    <a href=â€???â€ target=â€_blankâ€>Final presentation</a>

</body>

</html>
