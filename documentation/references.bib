@inproceedings{taylor2007cross,
 author = {Taylor, Matthew E. and Stone, Peter},
 title = {Cross-domain Transfer for Reinforcement Learning},
 booktitle = {Proceedings of the 24th International Conference on Machine Learning},
 series = {ICML '07},
 year = {2007},
 isbn = {978-1-59593-793-3},
 location = {Corvalis, Oregon, USA},
 pages = {879--886},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1273496.1273607},
 doi = {10.1145/1273496.1273607},
 acmid = {1273607},
 publisher = {ACM},
 address = {New York, NY, USA},
}


@inproceedings{taylor2008autonomous,
 author = {Taylor, Matthew E. and Kuhlmann, Gregory and Stone, Peter},
 title = {Autonomous Transfer for Reinforcement Learning},
 booktitle = {Proceedings of the 7th International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 1},
 series = {AAMAS '08},
 year = {2008},
 isbn = {978-0-9817381-0-9},
 location = {Estoril, Portugal},
 pages = {283--290},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=1402383.1402427},
 acmid = {1402427},
 publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
 address = {Richland, SC},
 keywords = {reinforcement learning, transfer learning},
}


@article{gupta2017learning,
  title={Learning invariant feature spaces to transfer skills with reinforcement learning},
  author={Gupta, Abhishek and Devin, Coline and Liu, YuXuan and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1703.02949},
  year={2017}
}

@article{MAESN,
    author    = {Abhishek Gupta and
    Russell Mendonca and
    Yuxuan Liu and
    Pieter Abbeel and
    Sergey Levine},
    title     = {Meta-Reinforcement Learning of Structured Exploration Strategies},
    journal   = {CoRR},
    volume    = {abs/1802.07245},
    year      = {2018},
    url       = {http://arxiv.org/abs/1802.07245},
    archivePrefix = {arXiv},
    eprint    = {1802.07245},
    timestamp = {Mon, 13 Aug 2018 16:47:04 +0200},
    biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1802-07245},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{parisotto2015actor,
  title={Actor-mimic: Deep multitask and transfer reinforcement learning},
  author={Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1511.06342},
  year={2015}
}

@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016}
}

@inproceedings{andrychowicz2017hindsight,
  title={Hindsight experience replay},
  author={Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, OpenAI Pieter and Zaremba, Wojciech},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5048--5058},
  year={2017}
}

@inproceedings{ng1999policy,
  title={Policy invariance under reward transformations: Theory and application to reward shaping},
  author={Ng, Andrew Y and Harada, Daishi and Russell, Stuart},
  booktitle={ICML},
  volume={99},
  pages={278--287},
  year={1999}
}

@article{popov2017data,
  title={Data-efficient deep reinforcement learning for dexterous manipulation},
  author={Popov, Ivaylo and Heess, Nicolas and Lillicrap, Timothy and Hafner, Roland and Barth-Maron, Gabriel and Vecerik, Matej and Lampe, Thomas and Tassa, Yuval and Erez, Tom and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1704.03073},
  year={2017}
}

@article{MAML,
    author    = {Chelsea Finn and
    Pieter Abbeel and
    Sergey Levine},
    title     = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
    journal   = {CoRR},
    volume    = {abs/1703.03400},
    year      = {2017},
    url       = {http://arxiv.org/abs/1703.03400},
    archivePrefix = {arXiv},
    eprint    = {1703.03400},
    timestamp = {Mon, 13 Aug 2018 16:47:43 +0200},
    biburl    = {https://dblp.org/rec/bib/journals/corr/FinnAL17},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{goldberg2017neural,
 author = {Goldberg, Yoav and Hirst, Graeme},
 title = {Neural Network Methods in Natural Language Processing},
 year = {2017},
 isbn = {1627052984, 9781627052986},
 publisher = {Morgan \& Claypool Publishers},
}

@book{goodfellow2016deep,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{hinton2006reducing,
  title={Reducing the dimensionality of data with neural networks},
  author={Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
  journal={science},
  volume={313},
  number={5786},
  pages={504--507},
  year={2006},
  publisher={American Association for the Advancement of Science}
}

@techreport{deep-learning-methods-and-applications,
    author = {Deng, Li and Yu, Dong},
    title = {Deep Learning: Methods and Applications},
    booktitle = {},
    year = {2014},
    month = {May},
    abstract = {
    
    This book is aimed to provide an overview of general deep learning methodology and its applications to a variety of signal and information processing tasks. The application areas are chosen with the following three criteria: 1) expertise or knowledge of the authors; 2) the application areas that have already been transformed by the successful use of deep learning technology, such as speech recognition and computer vision; and 3) the application areas that have the potential to be impacted significantly by deep learning and that have gained concentrated research efforts, including natural language and text processing, information retrieval, and multimodal information processing empowered by multi-task deep learning.
    
    In Chapter 1, we provide the background of deep learning, as intrinsically connected to the use of multiple layers of nonlinear transformations to derive features from the sensory signals such as speech and visual images. In the most recent literature, deep learning is embodied also as representation learning, which involves a hierarchy of features or concepts where higher-level representations of them are defined from lower-level ones and where the same lower-level representations help to define higher-level ones. In Chapter 2, a brief historical account of deep learning is presented. In particular, selected chronological development of speech recognition is used to illustrate the recent impact of deep learning that has become a dominant technology in speech recognition industry within only a few years since the start of a collaboration between academic and industrial researchers in applying deep learning to speech recognition. In Chapter 3, a three-way classification scheme for a large body of work in deep learning is developed. We classify a growing number of deep learning techniques into unsupervised, supervised, and hybrid categories, and present qualitative descriptions and a literature survey for each category. From Chapter 4 to Chapter 6, we discuss in detail three popular deep networks and related learning methods, one in each category. Chapter 4 is devoted to deep autoencoders as a prominent example of the unsupervised deep learning techniques. Chapter 5 gives a major example in the hybrid deep network category, which is the discriminative feed-forward neural network for supervised learning with many layers initialized using layer-by-layer generative, unsupervised pre-training. In Chapter 6, deep stacking networks and several of the variants are discussed in detail, which exemplify the discriminative or supervised deep learning techniques in the three-way categorization scheme.
    
    In Chapters 7-11, we select a set of typical and successful applications of deep learning in diverse areas of signal and information processing and of applied artificial intelligence. In Chapter 7, we review the applications of deep learning to speech and audio processing, with emphasis on speech recognition organized according to several prominent themes. In Chapters 8, we present recent results of applying deep learning to language modeling and natural language processing. Chapter 9 is devoted to selected applications of deep learning to information retrieval including Web search. In Chapter 10, we cover selected applications of deep learning to image object recognition in computer vision. Selected applications of deep learning to multi-modal processing and multi-task learning are reviewed in Chapter 11. Finally, an epilogue is given in Chapter 12 to summarize what we presented in earlier chapters and to discuss future challenges and directions.
    
    
    },
    publisher = {NOW Publishers},
    url = {https://www.microsoft.com/en-us/research/publication/deep-learning-methods-and-applications/},
    address = {},
    pages = {},
    journal = {},
    volume = {},
    chapter = {},
    isbn = {},
}

@article{zhou2017optimizing,
author = {Zhou, Zhenpeng and Li, Xiaocheng and Zare, Richard N.},
title = {Optimizing Chemical Reactions with Deep Reinforcement Learning},
journal = {ACS Central Science},
volume = {3},
number = {12},
pages = {1337-1344},
year = {2017},
doi = {10.1021/acscentsci.7b00492},
note ={PMID: 29296675},
URL = {https://doi.org/10.1021/acscentsci.7b00492},
eprint = {https://doi.org/10.1021/acscentsci.7b00492}
}

@article{silver2017mastering,
author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and Driessche, George van den and Graepel, Thore and Hassabis, Demis},
  year = {2017},
  title = {Mastering the game of Go without human knowledge},
  journal = {Nature},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature24270},
  volume = {550},
  month = {10},
  pages = {354},
  number = {7676},
  url = {http:https://doi.org/10.1038/nature24270},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.}
}

@article{russell2016research,
  author    = {Stuart J. Russell and
               Daniel Dewey and
               Max Tegmark},
  title     = {Research Priorities for Robust and Beneficial Artificial Intelligence},
  journal   = {CoRR},
  volume    = {abs/1602.03506},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.03506},
  archivePrefix = {arXiv},
  eprint    = {1602.03506},
  timestamp = {Mon, 13 Aug 2018 16:48:35 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/RussellDT16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article {Hinton504,
    author = {Hinton, G. E. and Salakhutdinov, R. R.},
    title = {Reducing the Dimensionality of Data with Neural Networks},
    volume = {313},
    number = {5786},
    pages = {504--507},
    year = {2006},
    doi = {10.1126/science.1127647},
    publisher = {American Association for the Advancement of Science},
    abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such {\textquotedblleft}autoencoder{\textquotedblright} networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
    issn = {0036-8075},
    URL = {http://science.sciencemag.org/content/313/5786/504},
    eprint = {http://science.sciencemag.org/content/313/5786/504.full.pdf},
    journal = {Science}
}

@article{2016arXiv160605579H,
    author = {{Higgins}, I. and {Matthey}, L. and {Glorot}, X. and {Pal}, A. and 
    {Uria}, B. and {Blundell}, C. and {Mohamed}, S. and {Lerchner}, A.
    },
    title = "{Early Visual Concept Learning with Unsupervised Deep Learning}",
    journal = {ArXiv e-prints},
    archivePrefix = "arXiv",
    eprint = {1606.05579},
    primaryClass = "stat.ML",
    keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition},
    year = 2016,
    month = jun,
    adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160605579H},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@misc{openaigym,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}