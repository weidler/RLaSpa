\section{Discussion}
\label{sec:discussion}

% (discussion of results)
As indicated by the experiments on isolated modules, both the policy learner and the representation module are working reasonably well on their own. While the decoding in Janus and Cerberus where certainly imperfect, this issue should be surmountable by the full backpropagation mechanism. However, when the pre-training on the Tunnel or the Scrollers Task is transferred to a training process on the Race task, we do not only observe no improvement in training. In fact, the pre-trained agent is incapable of learning a useful policy during a number of episodes that is sufficient for a randomly initialized agent. We expect this to be attributable to the following reasons:

\begin{enumerate}[i]
    \item The task we chose for pre-training turned out to be the one that the isolated agent could not learn to perform either. While we believe that transfer from Tunnel to any other task is most interesting due to the drastic difference in the environment, easier transfer could be more successful. Nonetheless, we believe that a system that presumably can perform transfer should be able to transfer between Tunnel and other Scrollers/Race as well. As our analysis of the latent space revealed, the issue with the tunnel task is that the representation module does not focus on the task-relevant features.

    \item More generally speaking, the pre-training might set both the encoder and the policy learner into an area of the loss space, where the agent not only can make no use of the pre-trained policy network. It is also more difficult to reach a better loss, hence the distance to these deeper minimums is greater. Since it is the very purpose of transfer learning to find minimums that are close to those of other tasks, this means that our approach fails to generalize the latent space enough, which is also confirmed by the latent space analysis. The policy learned on the source task is consequently still too specific to allow any transfer.
\end{enumerate}

% discussion why our approach may be flawed/ what couldve been better
We assumed that capturing the dynamics of the environment the agent is acting in is helpful, but it appears that the proposed architectures do not sufficiently achieve this. The underlying idea behind the autoencoder's decoder that predicts the next state based on the encoding of the current state and the action is a reasonable measure to capture the dynamics. In the simple tasks investigated in this work though, the actions of the agent only affect its own position but not the dynamics of the environment. Furthermore, changes in the environment from one frame to another are always very small. This raises the question whether the reconstruction of the current state and the prediction of the next state are not basically the same objective, since the autoencoders cannot model details precisely enough to make such distinctions, particularly in the case of multi-task learning.

Previous work stack the previous $n$ (e.g. 4) frames to incorporate dynamics \citep{DQN, DDQN, DuelingDQN}. We consider this to be not helpful for the given tasks. Since the obstacles are always moving in one direction in all environments, the best action will always depend only on the current situation and not on previous frames. In fact, overlaying multiple frames may be harmful in these environments, because it hides or at least blurs information.

% discussion how our work differed from related work introduced before and why that results in different results

\subsection{Limitations}
In the following we will address potential limitations to the results presented in this work.

\paragraph{Training time} A factor concerning the poor performance of the agents on a single task could be that it was not trained long enough to find a good policy. Due to time limitations longer training was not possible.

\paragraph{Hyperparameter Tuning} Another reason for sub-optimal performance in any architecture presented in this work are non-optimal hyperparameter settings. These can be the exploration rate regarding the RL agent, but also any configuration of layer sizes and numbers for the representation module and the DDQN. Especially the latter can be crucial, since deep learning systems heavily depend on the correct setting of parameters. An exhaustive grid search was infeasible in the scope of this work, but we suggest that performance can be increased by doing a sophisticated optimization of the parameters.

\paragraph{Unstable Learning Process} It should also be noted, that repetitive training runs of the isolated DDQN system revealed unstable learning processes. That is, progress occasionally was made earlier or later than in the presented figures and tables. Furthermore, the performance repeatedly decreased after reaching intermediate peaks, because the agent reconsiders its current behavior to adapt to infrequent but harmful situations. This makes judgments about increased learning speed from transfer difficult in general. Given the drastic differences in our results though, we are confident in their interpretation.  